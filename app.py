import streamlit as st
import requests

# Load Hugging Face API key securely
HF_API_KEY = st.secrets["hugging_face_api_key"]

# List of books in the Bible
bible_books = [
    "×‘×¨××©×™×ª", "×©××•×ª", "×•×™×§×¨×", "×‘××“×‘×¨", "×“×‘×¨×™×",
    "×™×”×•×©×¢", "×©×•×¤×˜×™×", "×¨×•×ª", "×©××•××œ ×", "×©××•××œ ×‘",
    "××œ×›×™× ×", "××œ×›×™× ×‘", "×™×©×¢×™×”×•", "×™×¨××™×”×•", "×™×—×–×§××œ",
    "×”×•×©×¢", "×™×•××œ", "×¢××•×¡", "×¢×•×‘×“×™×”", "×™×•× ×”",
    "××™×›×”", "× ×—×•×", "×—×‘×§×•×§", "×¦×¤× ×™×”", "×—×’×™",
    "×–×›×¨×™×”", "××œ××›×™", "×ª×”×™×œ×™×", "××©×œ×™", "××™×•×‘",
    "×©×™×¨ ×”×©×™×¨×™×", "××™×›×”", "×§×”×œ×ª", "××¡×ª×¨",
    "×“× ×™××œ", "×¢×–×¨×", "× ×—××™×”", "×“×‘×¨×™ ×”×™××™× ×", "×“×‘×¨×™ ×”×™××™× ×‘"
]

# Streamlit UI
st.set_page_config(page_title="Biblical Hebrew Quiz Generator", layout="centered")
st.title("ğŸ“œ Biblical Hebrew Reading Comprehension Quiz")

book = st.selectbox("ğŸ“– Select Book of the Bible:", bible_books)
chapter = st.number_input("ğŸ“„ Chapter Number:", min_value=1, step=1)
num_questions = st.slider("ğŸ”¢ Number of Questions", min_value=3, max_value=10, value=5)

if st.button("Generate Quiz"):
    with st.spinner("Generating quiz..."):
        try:
            prompt = f"""
××ª×” ××•×¨×” ×œ×œ×©×•×Ÿ ××§×¨××™×ª. ×›×ª×•×‘ ×©××œ×•×Ÿ ×”×‘× ×ª ×”× ×§×¨× ×¢×œ ×¤×¨×§ {chapter} ××ª×•×š ×¡×¤×¨ {book}.
×”×©××œ×•×Ÿ ×¦×¨×™×š ×œ×›×œ×•×œ {num_questions} ×©××œ×•×ª.
×”×©×ª××© ×‘×¢×‘×¨×™×ª ××§×¨××™×ª ×‘×œ×‘×“ (×›×•×œ×œ × ×™×§×•×“ ××œ×), ×©××œ ×©××œ×•×ª ×¤×¨×˜× ×™×•×ª ×¢×œ ×ª×•×›×Ÿ ×”×¤×¨×§.
×”×¦×’ ×›×œ ×©××œ×” ×‘×¦×•×¨×” ×©×œ ×©××œ×” ×××™×ª×™×ª, ×•×›×œ ×ª×©×•×‘×” ×›××¤×©×¨×•×ª ××©×¤×˜×™×ª, ×œ× ×›×©××œ×”.
×¢×‘×•×¨ ×›×œ ×©××œ×” ×”×¦×’ ××¨×‘×¢ ××¤×©×¨×•×™×•×ª â€“ ×¨×§ ××—×ª ××”×Ÿ × ×›×•× ×”.
××œ ×ª×¦×™×’ ××ª ×”×¤×¡×•×§×™× ×¢×¦××.
"""

            # Call Hugging Face inference API
            headers = {
                "Authorization": f"Bearer {HF_API_KEY}"
            }

            payload = {
                "inputs": prompt,
                "parameters": {"max_new_tokens": 700, "temperature": 0.7},
                "options": {"wait_for_model": True}
            }

            response = requests.post(
                "https://api-inference.huggingface.co/models/bigscience/bloom-1b1",  # You can change to another model
                headers=headers,
                json=payload
            )

            result = response.json()

            if "error" in result:
                st.error(f"âŒ API Error: {result['error']}")
            else:
                generated_text = result[0]["generated_text"]
                # Extract only the part after the prompt
                quiz_text = generated_text[len(prompt):].strip()
                st.markdown("### âœï¸ ×”××‘×—×Ÿ ×©×œ×š:")
                st.markdown(quiz_text)

        except Exception as e:
            st.error(f"âŒ Error generating quiz:\n\n{e}")
